{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan of the Chapter \n",
    "\n",
    "[1. Using Multiple Features](#section1) \n",
    ">  [1. Model building with Statsmodels](#section11)  \n",
    "   [2. Using Formulas as an Alternative](#section12)  \n",
    "   [3. The Correlation Matrix](#section13)  \n",
    "\n",
    "[2. Revisiting Gradient Descent](#section2) \n",
    ">  [4. Feature Scaling](#section21)  \n",
    "   [5. Unstandardizing Coefficients](#section22)  \n",
    "\n",
    "[3. Estimating Features Importance](#section3)\n",
    ">  [3.1. Inspecting Standardized Coefficients](#section31)  \n",
    "   [3.2. Comparing Models by R-squared](#section32) \n",
    "\n",
    "[4. Interaction Models](#section4)\n",
    ">  [4.1. Discovering Interactions](#section41)  \n",
    "\n",
    "[5. Polynomial Regression](#section5)\n",
    ">  [5.1. Testing Linear Versus Cubic Transformation](#section51)  \n",
    "   [5.2. Going for Higher-Degree Solutions](#section52)  \n",
    "   [5.3. Introducing Underfitting and Overfitting](#section53)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import linear_model\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline \n",
    "from matplotlib import rc\n",
    "\n",
    "rc('font', **{'family': 'sans-serif', 'sans-serif': ['Computer Modern Roman']})\n",
    "params = {'axes.labelsize': 12,\n",
    "          'font.size': 14,\n",
    "          'legend.fontsize':12,\n",
    "          'xtick.labelsize': 12,\n",
    "          'ytick.labelsize': 12,\n",
    "          'text.usetex': True,\n",
    "          'figure.figsize': (14, 8)\n",
    "          }\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"section1\"></a> \n",
    "# 1. Using Multiple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boston**  dataset that tries to explain different house prices in the Boston of the 70s, given a series of statistics aggregated at the census zone level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Features\n",
    "boston = load_boston()\n",
    "dataset = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "## Target\n",
    "dataset['target'] = boston.target\n",
    "n_samples = len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will always work by keeping with us a series of informative variables, the number of observation and variable names, the input data matrix, and the response vector at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data as arrays\n",
    "features = dataset.columns[:-1]\n",
    "X = dataset.iloc[:, :-1]\n",
    "y = dataset['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section11\"></a> \n",
    "## 1.1.  Model Building with Statsmodels\n",
    "\n",
    "\n",
    "As a first step toward extending to more predictors the previously done analysis with Statsmodels, let's reload the necessary modules from the package (one working with matrices and the other with formulas).\n",
    "\n",
    "Let's also prepare a suitable input matrix, naming it **Xc** after having it incremented by an extra column containing the bias vector (a constant variable having the unit value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzalaya/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "Xc = sm.add_constant(X)\n",
    "linear_regression = sm.OLS(y, Xc)\n",
    "fitted_model = linear_regression.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having fitted the preceding specified model, let's immediately ask for a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.741</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.734</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   108.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 27 Oct 2019</td> <th>  Prob (F-statistic):</th> <td>6.72e-135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:50:48</td>     <th>  Log-Likelihood:    </th> <td> -1498.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3026.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   492</td>      <th>  BIC:               </th> <td>   3085.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>   36.4595</td> <td>    5.103</td> <td>    7.144</td> <td> 0.000</td> <td>   26.432</td> <td>   46.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>    <td>   -0.1080</td> <td>    0.033</td> <td>   -3.287</td> <td> 0.001</td> <td>   -0.173</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>      <td>    0.0464</td> <td>    0.014</td> <td>    3.382</td> <td> 0.001</td> <td>    0.019</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>INDUS</th>   <td>    0.0206</td> <td>    0.061</td> <td>    0.334</td> <td> 0.738</td> <td>   -0.100</td> <td>    0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>    <td>    2.6867</td> <td>    0.862</td> <td>    3.118</td> <td> 0.002</td> <td>    0.994</td> <td>    4.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>     <td>  -17.7666</td> <td>    3.820</td> <td>   -4.651</td> <td> 0.000</td> <td>  -25.272</td> <td>  -10.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>      <td>    3.8099</td> <td>    0.418</td> <td>    9.116</td> <td> 0.000</td> <td>    2.989</td> <td>    4.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>     <td>    0.0007</td> <td>    0.013</td> <td>    0.052</td> <td> 0.958</td> <td>   -0.025</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>     <td>   -1.4756</td> <td>    0.199</td> <td>   -7.398</td> <td> 0.000</td> <td>   -1.867</td> <td>   -1.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>     <td>    0.3060</td> <td>    0.066</td> <td>    4.613</td> <td> 0.000</td> <td>    0.176</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>     <td>   -0.0123</td> <td>    0.004</td> <td>   -3.280</td> <td> 0.001</td> <td>   -0.020</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th> <td>   -0.9527</td> <td>    0.131</td> <td>   -7.283</td> <td> 0.000</td> <td>   -1.210</td> <td>   -0.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>       <td>    0.0093</td> <td>    0.003</td> <td>    3.467</td> <td> 0.001</td> <td>    0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>   <td>   -0.5248</td> <td>    0.051</td> <td>  -10.347</td> <td> 0.000</td> <td>   -0.624</td> <td>   -0.425</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>178.041</td> <th>  Durbin-Watson:     </th> <td>   1.078</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 783.126</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.521</td>  <th>  Prob(JB):          </th> <td>8.84e-171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.281</td>  <th>  Cond. No.          </th> <td>1.51e+04</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.51e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.741\n",
       "Model:                            OLS   Adj. R-squared:                  0.734\n",
       "Method:                 Least Squares   F-statistic:                     108.1\n",
       "Date:                Sun, 27 Oct 2019   Prob (F-statistic):          6.72e-135\n",
       "Time:                        20:50:48   Log-Likelihood:                -1498.8\n",
       "No. Observations:                 506   AIC:                             3026.\n",
       "Df Residuals:                     492   BIC:                             3085.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         36.4595      5.103      7.144      0.000      26.432      46.487\n",
       "CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043\n",
       "ZN             0.0464      0.014      3.382      0.001       0.019       0.073\n",
       "INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141\n",
       "CHAS           2.6867      0.862      3.118      0.002       0.994       4.380\n",
       "NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262\n",
       "RM             3.8099      0.418      9.116      0.000       2.989       4.631\n",
       "AGE            0.0007      0.013      0.052      0.958      -0.025       0.027\n",
       "DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084\n",
       "RAD            0.3060      0.066      4.613      0.000       0.176       0.436\n",
       "TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005\n",
       "PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696\n",
       "B              0.0093      0.003      3.467      0.001       0.004       0.015\n",
       "LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425\n",
       "==============================================================================\n",
       "Omnibus:                      178.041   Durbin-Watson:                   1.078\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126\n",
       "Skew:                           1.521   Prob(JB):                    8.84e-171\n",
       "Kurtosis:                       8.281   Cond. No.                     1.51e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.51e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **adjusted R-squared** considers the complexity of the model and reports a much more realistic **R-squared measure**. \n",
    "\n",
    "Variables with a low **p-value** are good candidates for being removed from the model because there will probably be little proof that their estimated coefficient is different from zero.\n",
    "\n",
    "In our example, being largely not significant (**p-value** major of 0.05), the **AGE** and **INDUS** variables are represented in the model by coefficients whose usefulness could be seriously challenged.\n",
    "\n",
    "The condition number test **Cond. No.** is another statistic that now acquires a fresh importance under the light of a system of predictors. \n",
    "It signals numeric unstable results when trying an optimization based on matrix inversion. \n",
    "The cause of such instability is due to multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section12\"></a>\n",
    "## 1.2. Using Formulas as an Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = smf.ols(formula = 'target ~ CRIM + ZN + '\n",
    "                                      'INDUS + CHAS + NOX + '\n",
    "                                      'RM + AGE + DIS + RAD +'\n",
    "                                      ' TAX + PTRATIO + B + LSTAT', data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = linear_regression.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section13\"></a>\n",
    "## 1.3 The Correlation Matrix\n",
    "\n",
    "To determine the manner and number of predictors affecting each other, the right tool is a correlation matrix, which, though a bit difficult to read when the number of the features is high, is still the most direct way to ascertain the presence of shared variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
      "CRIM     1.000000 -0.200469  0.406583 -0.055892  0.420972 -0.219247  0.352734   \n",
      "ZN      -0.200469  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
      "INDUS    0.406583 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
      "CHAS    -0.055892 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
      "NOX      0.420972 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
      "RM      -0.219247  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
      "AGE      0.352734 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
      "DIS     -0.379670  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
      "RAD      0.625505 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
      "TAX      0.582764 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
      "PTRATIO  0.289946 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
      "B       -0.385064  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
      "LSTAT    0.455621 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
      "\n",
      "              DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
      "CRIM    -0.379670  0.625505  0.582764  0.289946 -0.385064  0.455621  \n",
      "ZN       0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  \n",
      "INDUS   -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800  \n",
      "CHAS    -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  \n",
      "NOX     -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879  \n",
      "RM       0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  \n",
      "AGE     -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339  \n",
      "DIS      1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  \n",
      "RAD     -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676  \n",
      "TAX     -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993  \n",
      "PTRATIO -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044  \n",
      "B        0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  \n",
      "LSTAT   -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000  \n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = X.corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, some high correlations appear to be present in the order of the absolute value of 0.70 between **TAX**, **NOX**, **INDUS**, and **DIS**. \n",
    "That's fairly explainable since **DIS** is the distance from employment centers, **NOX** is a pollution indicator, **INDUS** is the quota of non-residential or commercial buildings in the area, and **TAX** is the property tax rate. \n",
    "The right combination of these variables can well hint at what the productive areas are.\n",
    "\n",
    "A faster, but less numerical representation is to build a heat map of the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrix correlation\n",
    "def visualize_correlation_matrix(data, hurdle=0.0):\n",
    "    R = np.corrcoef(data, rowvar=0)\n",
    "    R[np.where(np.abs(R) < hurdle)] = 0.0\n",
    "    heatmap = plt.pcolor(R, cmap=mpl.cm.coolwarm, alpha=0.8)\n",
    "    heatmap.axes.set_frame_on(False)\n",
    "    heatmap.axes.set_yticks(np.arange(R.shape[0]) + 0.5, minor=False)\n",
    "    heatmap.axes.set_xticks(np.arange(R.shape[1]) + 0.5, minor=False)\n",
    "    heatmap.axes.set_xticklabels(features, minor=False)\n",
    "    plt.xticks(rotation=90)\n",
    "    heatmap.axes.set_yticklabels(features, minor=False)\n",
    "    plt.tick_params(axis='both', which='both', bottom='off', \n",
    "                    top='off', left='off', right='off')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_correlation_matrix(X, hurdle=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heat map immediately reveals how **PTRATIO** and **B** are not so related to other predictors. \n",
    "Another intuition provided by the map is that a cluster of variables, namely **TAX**, **INDUS**, **NOX**, and **RAD**, is confirmed to be in strong linear association.\n",
    "\n",
    "An even more automatic way to detect such associations (and figure out numerical problems in a matrix inversion) is to use eigenvectors. \n",
    "\n",
    "Eigenvectors are a very smart way to recombine the variance among the variables, creating new features accumulating all the shared variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.12684883 1.43327512 1.24261667 0.85757511 0.83481594 0.65740718\n",
      " 0.53535609 0.39609731 0.06350926 0.27694333 0.16930298 0.18601437\n",
      " 0.22023782]\n"
     ]
    }
   ],
   "source": [
    "corr = np.corrcoef(X, rowvar=0)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(corr)\n",
    "print(eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the eigenvalues, we print them in descending order and look for any element whose value is near to zero or small compared to the others.\n",
    "\n",
    "Near zero values can represent a real problem for normal equations and other optimization methods based on matrix inversion.\n",
    "\n",
    "Small values represent a high but not critical source of multicollinearity. \n",
    "\n",
    "If you spot any of these low values, keep a note of their index in the list (Python indexes start from zero). \n",
    "\n",
    "Using their index position in the list of eigenvalues, you can recall their specific vector from eigenvectors, which contains all the variable loadings—that is, the level of association with the original variables. \n",
    "\n",
    "In our example, we investigate the eigenvector at index 8. Inside the eigenvector, we notice values at index positions 2, 8, and 9, which are indeed outstanding in terms of absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0459523   0.08091897  0.25107654 -0.03592171 -0.04363045 -0.0455671\n",
      "  0.03855068  0.01829854  0.63348972 -0.72023345 -0.02339805  0.00446307\n",
      " -0.02443168]\n"
     ]
    }
   ],
   "source": [
    "print(eigenvectors[:,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now print the variables' names to know which ones contribute so much by their values to build the eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDUS RAD TAX\n"
     ]
    }
   ],
   "source": [
    "print(features[2], features[8], features[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having found the multicollinearity culprits, what remedy could we use for such variables? Removal of some of them is usually the best solution and that will be carried out in an automated way when exploring how variable selection works in Chapter 6, Achieving Generalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section2\"></a>\n",
    "# 2. Revisiting Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section21\"></a>\n",
    "## 2.1. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Working with different features requires more attention when estimating the coefficients because of their similarities which can cause a variance increase of the estimates. \n",
    "\n",
    "Multicollinearity between variables also has other drawbacks because it can also make matrix inversion (the matrix operation at the core of the normal equation coefficient estimation) very difficult, if not impossible, to achieve; such a problem is due to a mathematical limitation of the algorithm.\n",
    " \n",
    "Anyway, though being quite resistant to problems that affect other approaches, its simplicity also makes it vulnerable to other common problems, such as the different scale present in each feature. In fact, some features in your data may be represented by measurements in units, some in decimals, and others in thousands, depending on what aspect of reality each feature represents. \n",
    "\n",
    "When it is the case that the features have a different scale, though the algorithm will be processing each of them separately, optimization will be dominated by the variables with the more extensive scale. Working in a space of dissimilar dimensions will require more iterations before convergence to a solution (and sometimes there might be no convergence at all).\n",
    "\n",
    "The remedy is very easy; it is just necessary to put all the features on the same scale. Such an operation is called **feature scaling**. Feature scaling can be achieved through **standardization** or **normalization**. \n",
    "\n",
    "Normalization rescales all the values in the interval between zero and one (usually, but different ranges are also possible), whereas standardization operates by removing the mean and dividing by standard deviation to obtain a unit variance. \n",
    "\n",
    "In our case, standardization is preferable, both because it easily permits retuning the obtained standardized coefficients into their original scale and also because, by centering all the features at the zero mean, it makes the error surface more tractable by many machine learning algorithms, in a much more effective way than just rescaling the maximum and minimum of a variable. \n",
    "\n",
    "\n",
    "Let's try the algorithm, first using standardization based on the **Scikit-learn preprocessing module**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "n_samples = len(dataset)\n",
    "features = dataset.columns\n",
    "standardization = StandardScaler()\n",
    "Xst = standardization.fit_transform(X)\n",
    "original_means = standardization.mean_\n",
    "original_std = standardization.scale_\n",
    "Xst = np.column_stack((Xst, np.ones(n_samples)))\n",
    "y = dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code, we just standardized the variables using the **StandardScaler** class from **Scikit-learn**. This class can fit a data matrix, record its column means and standard deviations, and operate a transformation on itself, as well as on any other similar matrix, standardizing the column data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_beta(p):\n",
    "    return np.array([np.random.normal() for j in range(p)])\n",
    "\n",
    "def hypothesis(Xdesign, beta):\n",
    "    return np.dot(Xdesign, beta)\n",
    "\n",
    "def loss(X, beta, y):\n",
    "    return y - hypothesis(X, beta)\n",
    "    \n",
    "#def loss(Xdesign, beta, ytarget):\n",
    "    #return y - np.dot(Xdesign, beta)\n",
    "\n",
    "#def squared_loss(X, beta, y):\n",
    "    #return loss(X, beta, y) ** 2\n",
    "\n",
    "def squared_loss(Xdesign, beta, ytarget):\n",
    "    return (ytarget - np.dot(Xdesign, beta))** 2 / (2 * len(ytarget))\n",
    "\n",
    "def gradients(Xdesign, beta, ytarget):\n",
    "    gradients = list()\n",
    "    n = float(len(y))\n",
    "    for j in range(len(beta)):\n",
    "        gradients.append(np.sum(loss(Xdesign, beta, ytarget) * Xdesign[:, j]) / n)\n",
    "    return gradients\n",
    "\n",
    "def update(Xdesign, beta, ytarget, alpha=0.01):\n",
    "    return [t - alpha * grad for t, grad in zip(beta, gradients(Xdesign, beta, ytarget))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Optimize(Xdesign, ytarget, alpha=0.01, tol=10**-12, iterations=20000):\n",
    "    beta = random_beta(Xdesign.shape[1])\n",
    "    path = list() #[]\n",
    "    for k in range(iterations):\n",
    "        SSL = np.sum(squared_loss(Xdesign, beta, ytarget))\n",
    "        beta_new = update(Xdesign, beta, ytarget, alpha=alpha)\n",
    "        new_SSL = np.sum(squared_loss(Xdesign, beta_new, ytarget))\n",
    "        beta = beta_new\n",
    "        if k >= 5 and (new_SSL - SSL <= tol and new_SSL >= -tol):\n",
    "            path.append(new_SSL)\n",
    "            return beta, path\n",
    "        if k % (iterations /20) == 0:\n",
    "            path.append(new_SSL)\n",
    "        return beta, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are our final standardized coefficients: 1.4233, \\ -0.9912, \\ -0.2085, \\ -1.0908, \\ 1.2020, \\ 0.6999, \\ 1.6218, \\ -0.3461, \\ -1.1540, \\ -0.7616, \\ 0.7345, \\ 1.3663, \\ 0.8198, \\ 1.8520\n"
     ]
    }
   ],
   "source": [
    "alpha =0.02\n",
    "beta, path = Optimize(Xst, y, alpha, tol=10**-12, iterations =1000)\n",
    "print(\"These are our final standardized coefficients: \" + \", \\ \" .join(map(lambda x: \"%0.4f\" % x, beta)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the output, we now need a way to rescale the coefficients to their variables' characteristics and we will be able to report the gradient descent solution in unstandardized form. \n",
    "\n",
    "Another point to mention is our choice of alpha. After some tests, the value of 0.02 has been chosen for its good performance on this very specific problem. Alpha is the **learning rate** and during optimization it can be fixed or changeable, accordingly to a **line search** method, modifying its value in order to minimize as far as possible the cost function at each single step of the optimization process. In our example, we opted for a fixed learning rate and we had to look for its best value by trying a few optimization values and deciding on which minimized the cost in the minor number of iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section22\"></a>\n",
    "## 2.2. Unstandardizing Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Given a vector of standardized coefficients from a linear regression and its bias, we can recall the formulation of the linear regression, which is: \n",
    " \\HUGE$${y = \\beta_0 + \\sum_{j=1}^p x_j\\beta_j}.$$\n",
    "\n",
    "The previous formula, transforming the predictors using their mean and standard deviation, is actually equivalent (after a few calculations) to such an expression:\n",
    "\n",
    "$$ y = \\bigg(\\hat \\beta_0 - \\sum_{j=1}^p \\frac{\\bar{x_j}\\hat\\beta_j}{\\delta_j}\\bigg)\n",
    "+ \n",
    "\\sum_{j=1}^p \\frac{\\hat\\beta_j}{\\delta_j}{x_j},$$\n",
    "where $\\bar{x}$ represents the original mean and $\\delta$ the original standar deviation of the varibales.\n",
    "\n",
    "By comparing the different parts of the two formulas (the first parenthesis and the second summation), we can calculate the bias and coefficient  equivalents when transforming a standardized coefficient into an unstandardized one. Without replicating all the mathematical formulas, we can quickly implement them into Python code and immediately provide an application showing how such calculations can transform gradient descent coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bias: -22.9350\n",
      "    CRIM:   0.1656\n",
      "      ZN:  -0.0425\n",
      "   INDUS:  -0.0304\n",
      "    CHAS:  -4.2987\n",
      "     NOX:  10.3829\n",
      "      RM:   0.9971\n",
      "     AGE:   0.0577\n",
      "     DIS:  -0.1645\n",
      "     RAD:  -0.1327\n",
      "     TAX:  -0.0045\n",
      " PTRATIO:   0.3396\n",
      "       B:   0.0150\n",
      "   LSTAT:   0.1149\n"
     ]
    }
   ],
   "source": [
    "unstandardized_betas = beta[:-1] / original_std\n",
    "unstandardized_bias = beta[-1] - np.sum((original_means / original_std) * beta[:-1])\n",
    "print('%8s: %8.4f' % ('bias', unstandardized_bias))\n",
    "for beta, feature_name in zip(unstandardized_betas, features):\n",
    "          print('%8s: %8.4f' % (feature_name, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an output from the previous code snippet, you will get a list of coefficients identical to our previous estimations with both Scikit-learn and Statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-c516fd032350>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-c516fd032350>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <a name=\"section3\"></a>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<a name=\"section3\"></a>\n",
    "# Estimating Features Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section31\"></a>\n",
    "## 3.1. Inspecting Standardized Coefficients\n",
    "\n",
    "By standardizing variables, we place them under a similar scale where a unit is the standard deviation of the variable itself. \n",
    "\n",
    "After standardization, larger coefficients can be interpreted as major contributions to establishing the result (a weighted summation, so the result will resemble larger weights more closely). Using standardized coefficients, we can therefore confidently rank our variables and spot those contributing less.\n",
    "\n",
    "Let's proceed to an example using our Boston dataset. This time we will be using the **LinearRegression** method from **Scikit-learn** because we do not need to do a linear model for its statistical properties, but just a working model using a fast and scalable algorithm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = linear_model.LinearRegression(normalize=False, fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such an initialization, apart from the **fit_intercept** parameter that explicates the insertion of a bias into the design of model, the normalize option indicates whether we intend to rescale all the variables in the range between 0 and 1. Such a transformation is different from statistical standardization and we will omit it for the moment by setting it in a False state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "standardization = StandardScaler()\n",
    "Stand_coef_linear_reg = make_pipeline(standardization, linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stand_coef_linear_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the **StandardScaler** seen before, we also import from **Scikit-learn** the convenient **make_pipeline** wrapper, which allows us to establish a sequence of operations to be done automatically on our data before feeding it to the linear regression analysis. Now, the **Stand_coef_linear_reg** pipeline will execute a statistical standardization on data before regressing it, thus outputting standardized coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.fit(X, y)\n",
    "for coef, feature in sorted(zip(map(abs, linear_regression.coef_),\n",
    "                                dataset.columns[:-1]), reverse=True):\n",
    "    print(\"%6.3f\\t %s\"%(coef, feature))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we output the coefficients of the regression on unstandardized data. As seen before, the output seems dominated by the huge coefficient of the **NOX** variable, which overlooks (with its absolute value of about 17.8) minor coefficients of 3.8 and less. However, we may question whether it could be so after also standardizing the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stand_coef_linear_reg.fit(X, y)\n",
    "for coef, feature in sorted(zip(map(abs, Stand_coef_linear_reg.steps[1][1].coef_),\n",
    "                           dataset.columns[:-1]), reverse=True):\n",
    "    print(\"%6.3f \\t %s\"%(coef, feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having all the predictors on a similar scale now, we can easily provide a more realistic interpretation of each coefficient. Clearly, it appears that a unit change has more impact when it involves the variables **LSTAT, DIS, RM, RAD, and TAX**. **LSTAT** is the percentage of lower status population, and this aspect explains its relevancy.\n",
    "\n",
    "Using standardized scales has certainly pointed us at the most important variables but it is still not a complete overview of the predictive power of each variable because: \n",
    "\n",
    "Standardized coefficients represent how well the model works out its predictions because large coefficients heavily impact the resulting response: though having a large coefficient is certainly a hint of how important a variable is, it tells us just a part of the role that the variable plays in reducing the error of the estimates and in making our predictions more accurate.\n",
    "\n",
    "Standardized coefficients can be ranked but their unit, though similar in scale, is somehow abstract (the standard deviation of each variable) and relative to the data at hand (so we shouldn't compare the standardized coefficients of different datasets because their standard deviations could be different)\n",
    "\n",
    "One solution could be to integrate the importance estimate based on standardized coefficients, instead using some measure related to the error measure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section32\"></a>\n",
    "## 3.2. Comparing Models by R-squared\n",
    "\n",
    "From a general point of view, we can evaluate a model by comparing how better it does in respect of simple mean, and that's the coefficient of determination, **R-square**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "linear_regression = linear_model.LinearRegression(normalize=False, fit_intercept=True)\n",
    "\n",
    "def r2_est(X, y):\n",
    "    return r2_score(y, linear_regression.fit(X, y).predict(X))\n",
    "\n",
    "print('Baseline R2:%0.3f' % r2_est(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_impact = list()\n",
    "for j in range(X.shape[1]):\n",
    "    selection = [i for i in range(X.shape[1]) if i!=j]\n",
    "    r2_impact.append(((r2_est(X, y) - r2_est(X.values[:, selection], y)), dataset.columns[j]))\n",
    "\n",
    "for imp, varname in sorted(r2_impact, reverse=True):\n",
    "                     print(\"%6.3f %s\" %(imp, varname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section4\"></a>\n",
    "# 4. Interaction Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section41\"></a>\n",
    "## 4.1. Discovering Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "linear_regression = linear_model.LinearRegression(normalize=False, fit_intercept=True)\n",
    "create_interactions = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By degree parameter we define how many variables to put into the interaction, it being possible to have three or even more variables interact with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_est(X, y):\n",
    "    return r2_score(y, linear_regression.fit(X, y).predict(X))\n",
    "\n",
    "baseline = r2_est(X, y)\n",
    "print(\"Baseline R2: %0.3f\" % baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = create_interactions.fit_transform(X)\n",
    "main_effects = create_interactions.n_input_features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After recalling the baseline R-squared value, the code creates a new input data matrix using the fit_transform method, enriching the original data with the interaction effects of all the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, effect in enumerate(create_interactions.powers_[(main_effects):]):\n",
    "    # print(np.where(effect==1))\n",
    "    # print(features[I])\n",
    "    # print(features[effect])\n",
    "    # break\n",
    "    termA, termB = features[np.where(effect==1)[0]]\n",
    "    # print(termA, termB)\n",
    "    #termA, termB = features[effect==1]\n",
    "    increment= r2_est(Xi[:, list(range(0, main_effects)) + [main_effects + k]], y) - baseline\n",
    "    if increment > 0.01:\n",
    "        print(\"Adding intearaction %8s * %8s R2: %5.3f\" % (termA, termB, increment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = X\n",
    "Xi[\"interaction\"] = X[\"RM\"] * X[\"LSTAT\"]\n",
    "print(\"R2 of a model with RM*LSTAT interaction: %0.3f\" % r2_est(Xi, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section5\"></a>\n",
    "# 5. Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section51\"></a>\n",
    "## 5.1. Testing Linear Versus Cubic Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "linear_regression = linear_model.LinearRegression(normalize=False, fit_intercept=True)\n",
    "create_cubic = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "create_quadratic =PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "\n",
    "linear_predictor = make_pipeline(linear_regression)\n",
    "cubic_predictor =make_pipeline(create_cubic, linear_regression)\n",
    "quadratic_predictor = make_pipeline(create_quadratic, linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = 'LSTAT'\n",
    "x = dataset['LSTAT'].values.reshape((n_samples, 1))\n",
    "xt = np.arange(0, 50, 0.1).reshape((np.int(50/0.1), 1))\n",
    "x_range = [dataset[predictor].min(),dataset[predictor].max()]\n",
    "y_range = [dataset['target'].min(),dataset['target'].max()]\n",
    "\n",
    "scatter = dataset.plot(kind='scatter', x=predictor, y='target',\n",
    "                       xlim=x_range, ylim=y_range)\n",
    "regr_line = scatter.plot(xt, linear_predictor.fit(x,y).predict(xt),\n",
    "                         '-', color='red', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first fit is the linear one (a simple linear regression) and from the scatterplot we can notice that the line is not representing well the cloud of points relating to 'LSTAT' with the response; most likely we need a curve. Instead of testing a second degree transformation that will turn into a parabola, we immediately try a cubic transformation: using two bends, we should obtain a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = dataset.plot(kind='scatter', x=predictor, y='target', \n",
    "                         xlim=x_range, ylim=y_range)\n",
    "regr_line = scatter.plot(xt, cubic_predictor.fit(x,y).predict(xt), \n",
    "                         '-', color='red', linewidth=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graphic check confirms that now we have a more credible representation of how 'LSTAT' and the response relate. We question whether we cannot do better using even higher-degree transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section52\"></a>\n",
    "## 5.2. Going for Higher-Degree Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test a higher degree of polynomial transformations, we prepare a script that creates the expansion and reports its R-squared measure. We then try to plot the function with the highest degree in the series and have a look at how it fits the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "for d in [1, 2, 3, 4, 5, 15]:\n",
    "    create_poly = PolynomialFeatures(degree=d, interaction_only=False, include_bias=False)\n",
    "    poly =make_pipeline(create_poly, StandardScaler(), linear_regression)\n",
    "    model = poly.fit(x, y)\n",
    "    print(\"R2 degree - %2i polynomial:%0.3f \\t\" %(d, r2_score(y, model.predict(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticeably, there is a huge difference in the coefficient of determination between the linear model and the quadratic expansion (second-degree polynomial). The measure jumps from 0.544 to 0.641, a difference that increases up to 0.682 when reaching the fifth degree. Preceding to even higher degrees, the increment is not so astonishing, though it keeps on growing, reaching 0.695 when the degree is the fifteenth. As the latter is the best result in terms of coefficient of determination, having a look at the plot on the data cloud will reveal a not so smooth fit, as we can see with lower degrees of polynomial expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = dataset.plot(kind='scatter', x=predictor, y='target', xlim=x_range, ylim=y_range)\n",
    "regr_line = scatter.plot(xt, model.predict(xt), '-', color='red', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing closely the resulting curve, you will surely notice how, by such a high degree, the curve tends strictly to follow the distribution of points, going erratic when the density diminishes at the fringes of the range of the predictors' values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section53\"></a>\n",
    "## 5.3. Introducing Underfitting and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression offers us the right occasion for starting to talk about **model complexity**. \n",
    "We have not explicitly tested it, but you may already have got the feeling that, by increasing the degree of the polynomial expansion, you are going to always reap better fits. \n",
    "We say more: the more variables you have in your model, the better, until you will have such a large number of betas, likely equal or almost equal to the number of your observations, that your predictions will be perfect.\n",
    "\n",
    "Decaying performances due to over-parameterization (an excess of parameters to be learned by the model) is a problem of linear regression and of many other machine learning algorithms. \n",
    "The more parameters you add, the better the fit because the model will cease to intercept the rules and regularities of your data but will start, in such an embarrassment of riches, to populate the many available coefficients with all the erratic and erroneous information present in the data. \n",
    "In such a situation, the model won't learn general rules but it will just be memorizing the dataset itself in another form. \n",
    "\n",
    "This is called **overfitting**: fitting the data at hand so well that the result is far from being an extraction of the form of the data to draw predictions from; the result is just a mere memorization. \n",
    "On the other side, another problem is **underfitting**—that is, when you are using too few parameters for your prediction. The most straightforward example is fitting a non-linear relation using a simple linear regression; clearly, it won't match the curve bends and some of its predictions will be misleading.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
